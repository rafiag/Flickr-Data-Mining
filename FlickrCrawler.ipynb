{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "FlickrCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Ks8slm775-",
        "colab_type": "text"
      },
      "source": [
        "#Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Det9Y67g8YZY",
        "colab_type": "text"
      },
      "source": [
        "#####Importing libraries and mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3jbexogepuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "f323147a-bbbd-4345-c84b-8cc5305ac424"
      },
      "source": [
        "import json\n",
        "import os, os.path\n",
        "import csv\n",
        "import requests\n",
        "from datetime import timedelta, date, datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjWfNl668dIG",
        "colab_type": "text"
      },
      "source": [
        "#####Defining function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH_YMqblepup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to automatically generate the date range from the parameter\n",
        "def daterange(start_date, end_date):\n",
        "    for n in range(int ((end_date - start_date).days)):\n",
        "        yield start_date + timedelta(n)\n",
        "        \n",
        "#function to help converting python dictionary to json file\n",
        "def python_dict_to_json_file(file_path):\n",
        "    try:\n",
        "        # Get a file object with write permission.\n",
        "        file_object = open(file_path, 'w')\n",
        "        #dict_object = dict(name='Tom', age=25, score=100)\n",
        "        # Save dict data into the JSON file.\n",
        "        json.dump(raw_data, file_object)\n",
        "        #print(file_path + ' created. ')\n",
        "    except FileNotFoundError:\n",
        "        print(file_path + ' not found. ')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vihs9NV18Mhe",
        "colab_type": "text"
      },
      "source": [
        "#####Setting parameter for crawling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34hOHXycepuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "de105e77-a8bb-44b5-c57e-99e971850bc2"
      },
      "source": [
        "root = '/content/drive/My Drive/Colab Notebooks/FlickrCrawling/Sarbagita'\n",
        "\n",
        "#defining parameter for the API\n",
        "api_key = '_________________________'\n",
        "year = '2020'\n",
        "minimum_taken_date = date(2019, 12, 1)\n",
        "maximum_taken_date = date(2020, 1, 1)\n",
        "lower_longitude = '114.416433'\n",
        "lower_latitude = '-8.875014'\n",
        "upper_longitude = '115.725141'\n",
        "upper_latitude = '-8.045447'\n",
        "bbox = lower_longitude+','+lower_latitude+','+upper_longitude+','+upper_latitude\n",
        "# bbox_south_west_point = '115.05,8.866667'\n",
        "# bbox_north_east_point = '115.3833,-8.433333'\n",
        "accuracy = '12' #World level is 1, Country is ~3, Region is ~6, City is ~11, Street is ~16\n",
        "has_geo = '1' #1 for geotagged photo, 0 for photo without geotag\n",
        "step = 2 #range of days between each data crawling attempt\n",
        "\n",
        "#defining date range for data\n",
        "date_range = []\n",
        "for single_date in daterange(minimum_taken_date, maximum_taken_date):\n",
        "    single_date_string = single_date.strftime('%Y-%m-%d')\n",
        "    date_range.append(single_date_string)\n",
        "date_length = len(date_range)\n",
        "print(date_length)\n",
        "print('date range: ' + date_range[0] + ' - ' + date_range[-1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "date range: 2019-12-01 - 2019-12-31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djjqXdo28su_",
        "colab_type": "text"
      },
      "source": [
        "#Data Crawling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCVz9A683Dr",
        "colab_type": "text"
      },
      "source": [
        "#####Using 'photos.search' function from to collect data matching the defined parameter and exporting the result into JSON files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKroriced9FF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "78f8db26-aaba-430f-abf9-2b908fa6277e"
      },
      "source": [
        "url = 'https://api.flickr.com/services/rest/?method=flickr.photos.search'\n",
        "\n",
        "i = 0\n",
        "j = step - 1\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        while j < date_length:\n",
        "            params = dict(api_key=api_key,\n",
        "                          format='json',\n",
        "                          nojsoncallback='1',\n",
        "                          min_taken_date=date_range[i],\n",
        "                          max_taken_date=date_range[j],\n",
        "                          bbox=bbox,\n",
        "                          accuracy=accuracy,\n",
        "                          has_geo=has_geo)\n",
        "            resp = requests.get(url=url, params=params)\n",
        "            data = json.loads(resp.text)\n",
        "\n",
        "            file_object = open(root+'/JSON/photos.search/{0}/{1} - {2}.json' .format(year, date_range[i], date_range[j]), 'w')\n",
        "            json.dump(data, file_object)\n",
        "\n",
        "            print('{0} - {1}.json created.' .format(date_range[i], date_range[j]))\n",
        "            # print(json.dumps(data, sort_keys=True, indent=4))\n",
        "            i += step\n",
        "            j += step\n",
        "    except Exception as ex:\n",
        "        print('{0} when trying to get data of {0} - {1}. Retrying...' .format(ex, date_range[i], date_range[j]))\n",
        "        continue\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-01 - 2019-12-02.json created.\n",
            "2019-12-03 - 2019-12-04.json created.\n",
            "2019-12-05 - 2019-12-06.json created.\n",
            "2019-12-07 - 2019-12-08.json created.\n",
            "2019-12-09 - 2019-12-10.json created.\n",
            "2019-12-11 - 2019-12-12.json created.\n",
            "2019-12-13 - 2019-12-14.json created.\n",
            "2019-12-15 - 2019-12-16.json created.\n",
            "2019-12-17 - 2019-12-18.json created.\n",
            "2019-12-19 - 2019-12-20.json created.\n",
            "2019-12-21 - 2019-12-22.json created.\n",
            "2019-12-23 - 2019-12-24.json created.\n",
            "2019-12-25 - 2019-12-26.json created.\n",
            "2019-12-27 - 2019-12-28.json created.\n",
            "2019-12-29 - 2019-12-30.json created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGjXdCpq9bd3",
        "colab_type": "text"
      },
      "source": [
        "#####Taking the 'photo_id', 'owner', and 'secret' from the previous JSON files and combining it into a python dictionaries list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emM-bml3epvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6b735c0b-3a38-47c2-c32e-eb1b5429e853"
      },
      "source": [
        "DIR = root+'/JSON/photos.search/{0}' .format(year)\n",
        "photos_search_count = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
        "x = 0\n",
        "i = 0\n",
        "j = step - 1\n",
        "dataset = []\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        while x < photos_search_count:\n",
        "            with open(root+'/JSON/photos.search/{0}/{1} - {2}.json' .format(year, date_range[i], date_range[j])) as data_file:\n",
        "                n = 0\n",
        "                jdata = json.load(data_file)\n",
        "                jdata_len = len(jdata['photos']['photo'])\n",
        "                while n < jdata_len:\n",
        "                    new_corpus = {'id':jdata['photos']['photo'][n]['id'],\n",
        "                                    'owner':jdata['photos']['photo'][n]['owner'],\n",
        "                                    'secret':jdata['photos']['photo'][n]['secret']}\n",
        "                    dataset.append(new_corpus)\n",
        "                    n += 1\n",
        "                #print('{0} corpus added from: {1} - {2}.json added.' .format(jdata_len, date_range[i], date_range[j]))\n",
        "                x += 1\n",
        "                i += step\n",
        "                j += step\n",
        "                n = 0\n",
        "    except Exception as ex:\n",
        "        print('{0} when trying to get data from to {1}. Retrying...' .format(ex, data_file))\n",
        "    break\n",
        "\n",
        "print('Total corpus added:', len(dataset))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expecting value: line 1 column 1 (char 0) when trying to get data from to <_io.TextIOWrapper name='/content/drive/My Drive/Colab Notebooks/FlickrCrawling/Sarbagita/JSON/photos.search/2020/2019-12-29 - 2019-12-30.json' mode='r' encoding='UTF-8'>. Retrying...\n",
            "Total corpus added: 449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsGqyybJ_k55",
        "colab_type": "text"
      },
      "source": [
        "#####Getting detailed data of each photo using the 'getInfo' function and data from the previous dictioary and exporting it to JSON files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZUhwLg4jpSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "27b881ef-87e3-4d47-c23d-49d81ab9121f"
      },
      "source": [
        "url = 'https://api.flickr.com/services/rest/?method=flickr.photos.getInfo'\n",
        "i = 0\n",
        "dataset_length = len(dataset)\n",
        "error_count = 0\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        while i < dataset_length:\n",
        "            params = dict(api_key=api_key,\n",
        "                          format='json',\n",
        "                          nojsoncallback='1',\n",
        "                          photo_id=dataset[i]['id'],\n",
        "                          secret=dataset[i]['secret'])\n",
        "            resp = requests.get(url=url, params=params)\n",
        "            data = json.loads(resp.text)\n",
        "            # print(json.dumps(data, sort_keys=True, indent=4))\n",
        "\n",
        "            file_object = open(root+'/JSON/getInfo/{0}/{1}.json' .format(year, dataset[i]['id']), 'w')\n",
        "            json.dump(data, file_object)\n",
        "            \n",
        "            i += 1\n",
        "            if i % 200 == 0:\n",
        "                print('{0} / {1} JSON data created.' .format(i, dataset_length))\n",
        "            error_count = 0\n",
        "    except Exception as ex:\n",
        "        error_count += 1\n",
        "        print('{0} when trying to get data of {1}. Retrying...' .format(ex, dataset[i]['id']))\n",
        "        if error_count > 5: pass\n",
        "        else: continue\n",
        "    break\n",
        "\n",
        "print('Total JSON data created: {0}' .format(i))  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200 / 449 JSON data created.\n",
            "400 / 449 JSON data created.\n",
            "Total JSON data created: 449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPVyL-VpABI2",
        "colab_type": "text"
      },
      "source": [
        "#####Taking the 'photo_id', 'owner_NSID', 'owner_location', 'date_taken', and 'location' from the previous json files and combining it into a single JSON file\n",
        "\n",
        "Note: Use [CodeBeautify](https://codebeautify.org/jsonviewer) to clean the JSON file and export it to csv format to make it easier to read and cleanse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9SW4Fc-e3SW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "47ef57bc-a532-4432-ebe8-563812869387"
      },
      "source": [
        "i = 0\n",
        "DIR = root+'/JSON/getInfo/{0}' .format(year)\n",
        "file_list = [f for f in os.listdir(DIR) if '.json' in f]\n",
        "num_files = len(file_list)\n",
        "csv_columns = ['photo_id', 'owner_nsid', 'owner_location', 'dates_taken', 'location_latitude', 'location_longitude', 'description']\n",
        "getinfo = []\n",
        "print('Files found in the directory: ', num_files)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # while i < num_files:\n",
        "        for f in file_list:\n",
        "            # with open(root+'/JSON/getInfo/{0}/{1}.json' .format(year, dataset[i]['id'])) as data_file:\n",
        "            with open(root+'/JSON/getInfo/{0}/{1}' .format(year, f)) as data_file:\n",
        "                jdata = json.load(data_file)\n",
        "                new_corpus = {'photo_id':jdata['photo']['id'],\n",
        "                              'owner_nsid':jdata['photo']['owner']['nsid'],\n",
        "                              'owner_location':jdata['photo']['owner']['location'],\n",
        "                              'dates_taken':jdata['photo']['dates']['taken'],\n",
        "                              'location_latitude':jdata['photo']['location']['latitude'],\n",
        "                              'location_longitude':jdata['photo']['location']['longitude'],\n",
        "                              'description':jdata['photo']['description']['_content']}\n",
        "                getinfo.append(new_corpus)\n",
        "                #print('Corpus from: {0}.json is added' .format(dataset[i]['id']))\n",
        "                i += 1\n",
        "                if i % 500 == 0:\n",
        "                    print('{0} / {1} files added to dictionary.' .format(i, num_files))\n",
        "                    continue\n",
        "    except Exception:\n",
        "        print('{0} when trying to get data from to {1}' .format(Exception, f))\n",
        "        # print('{0} when trying to get data from to {1}.json' .format(Exception, dataset[i]['id']))\n",
        "        pass\n",
        "    break\n",
        "\n",
        "# print('{0} / {1} files added to dictionary.' .format(i, num_files))\n",
        "\n",
        "try:\n",
        "    with open(root+'/CSV/dirty/{0}_dirty.csv' .format(year), 'w') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "        writer.writeheader()\n",
        "        for data in getinfo:\n",
        "            writer.writerow(data)\n",
        "except IOError:\n",
        "    print(\"I/O error\")\n",
        "\n",
        "print('A new CSV file containing {0} rows has been created.' .format(i))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files found in the directory:  514\n",
            "<class 'Exception'> when trying to get data from to 49285198212.json\n",
            "A new CSV file containing 456 rows has been created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zog4xJS33y",
        "colab_type": "text"
      },
      "source": [
        "#Download image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg1DF8btYvB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(root+'/CSV/2019-2020_ready.csv')\n",
        "df['description'] = ''\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        with open(root+'/JSON/getInfo/2019/{0}.json' .format(row['photo_id'])) as data_file: jdata = json.load(data_file)\n",
        "    except FileNotFoundError:\n",
        "        with open(root+'/JSON/getInfo/2020/{0}.json' .format(row['photo_id'])) as data_file: jdata = json.load(data_file)\n",
        "    df.at[i, 'description'] = jdata['photo']['description']['_content']\n",
        "    print(row['photo_id'], jdata['photo']['description']['_content'])\n",
        "\n",
        "# df.to_csv(root+'/CSV/2019-2020_ready(1).csv')\n",
        "# df.head()\n",
        "\n",
        "photo_id = df['photo_id']\n",
        "\n",
        "for idx in photo_id:\n",
        "    with open(root+'/JSON/getInfo/{0}/{1}.json' .format(year, idx)) as data_file:    \n",
        "        jdata = json.load(data_file)\n",
        "    url = jdata['photo']['urls']['url'][0]['_content']\n",
        "        \n",
        "    html = urlopen(url)\n",
        "    soup = BeautifulSoup(html)\n",
        "\n",
        "    img = soup.find('img', {'class':'low-res-photo'})\n",
        "    urllib.request.urlretrieve('https:'+img['src'], root+'/IMG/{0}/{1}_{2}.jpg' .format(year, jdata['photo']['dates']['taken'][:10], idx))\n",
        "\n",
        "    if idx % 100 == 0: print('{0} / {1} images downloaded.' .format(i, len(photo_id)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}